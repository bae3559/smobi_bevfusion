# BEVFusion Training Guide

This repository contains BEVFusion implementation for both **Waymo** and **nuScenes** datasets with complete dataset preparation, training, testing, and visualization pipelines.

## ğŸš€ Quick Start

### Requirements
- Python 3.8+
- CUDA 11.1+
- PyTorch 1.9+

### Installation
```bash
# Clone repository
git clone https://github.com/mit-han-lab/bevfusion.git
cd bevfusion

# Install dependencies
#pip install -r requirements.txt
python setup.py develop
```

## ğŸ“Š Dataset Preparation

### 1. Waymo Dataset

#### Download Data
```bash
# Download from Waymo Open Dataset
# Place raw data in: data/waymo/raw/
```

#### Convert to BEVFusion Format
```bash
# Convert Waymo data (full dataset)
python tools/create_data.py waymo \
    --root-path data/waymo/ \
    --out-dir data/waymo/Waymo_processed \
    --extra-tag waymo \
    --version full

# Convert Waymo mini dataset (for testing)
python tools/create_data.py waymo \
    --root-path data/waymo/Waymo_mini \
    --out-dir data/waymo/Waymo_processed \
    --extra-tag waymo \
    --version mini
```

#### Expected Directory Structure
```
data/waymo/
â”œâ”€â”€ Waymo_processed/
â”‚   â”œâ”€â”€ waymo_infos_train.pkl
â”‚   â”œâ”€â”€ waymo_infos_val.pkl
â”‚   â”œâ”€â”€ waymo_infos_test.pkl
â”‚   â””â”€â”€ waymo_dbinfos_train.pkl
â””â”€â”€ Waymo_mini/
    â”œâ”€â”€ training/
    â”œâ”€â”€ validation/
    â””â”€â”€ testing/
```

### 2. nuScenes Dataset

#### Download Data
```bash
# Download from nuScenes website
# Place data in: data/nuscenes/
```

#### Convert to BEVFusion Format
```bash
# Convert nuScenes data
python tools/create_data.py nuscenes \
    --root-path ./data/nuscenes \
    --out-dir ./data/nuscenes \
    --extra-tag nuscenes
```

#### Expected Directory Structure
```
data/nuscenes/
â”œâ”€â”€ maps/
â”œâ”€â”€ samples/
â”œâ”€â”€ sweeps/
â”œâ”€â”€ v1.0-trainval/
â”œâ”€â”€ nuscenes_infos_train.pkl
â”œâ”€â”€ nuscenes_infos_val.pkl
â””â”€â”€ nuscenes_dbinfos_train.pkl
```

## ğŸ‹ï¸ Training

### Waymo Training

#### 1. Single GPU Training
```bash
torchpack dist-run -np 1 python tools/train.py \
    configs/waymo/transfusion/secfpn/camera+lidar/swint_v0p075/default.yaml
```

#### 2. Multi-GPU Training
```bash
torchpack dist-run -np 8 python tools/train.py \
    configs/waymo/transfusion/secfpn/camera+lidar/swint_v0p075/default.yaml
```

#### 3. Configuration Options
- **Camera + LiDAR**: `configs/waymo/det/transfusion/secfpn/camera+lidar/default.yaml`
- **LiDAR Only**: `configs/waymo/det/transfusion/secfpn/lidar/default.yaml`
- **Camera Only**: `configs/waymo/det/centerhead/lssfpn/camera/default.yaml`

### nuScenes Training

#### 1. Single GPU Training
```bash
torchpack dist-run -np 1 python tools/train.py \
    configs/nuscenes/det/transfusion/secfpn/camera+lidar/swint_v0p075/convfuser.yaml \
    --model.encoders.camera.backbone.init_cfg.checkpoint pretrained/swint-nuimages-pretrained.pth \
    --load_from pretrained/lidar-only-det.pth 
```

#### 2. Multi-GPU Training
```bash
torchpack dist-run -np 8 python tools/train.py configs/nuscenes/det/transfusion/secfpn/camera+lidar/swint_v0p075/convfuser.yaml \
    --model.encoders.camera.backbone.init_cfg.checkpoint pretrained/swint-nuimages-pretrained.pth \
    --load_from pretrained/lidar-only-det.pth 
```

#### 3. Configuration Options
- **Camera + LiDAR**: `configs/nuscenes/det/transfusion/secfpn/camera+lidar/swint_v0p075/convfuser.yaml`
- **LiDAR Only**: `configs/nuscenes/det/transfusion/secfpn/lidar/voxelnet_0p075.yaml`
- **Camera Only**: `configs/nuscenes/det/centerhead/lssfpn/camera/256x704/swint/default.yaml`

### Training Options
```bash
# Resume from checkpoint
python tools/train.py CONFIG --resume-from CHECKPOINT

# Load pretrained weights
python tools/train.py CONFIG --load-from PRETRAINED

# Specify GPU IDs
CUDA_VISIBLE_DEVICES=0,1,2,3 python tools/train.py CONFIG
```

## ğŸ§ª Testing

### Waymo Testing

```bash
# Test with trained model
torchpack dist-run -np 1 python tools/test.py \
    runs/run-5e3259ec/configs.yaml \
    runs/run-5e3259ec/epoch_20.pth \
    --eval bbox

# Multi-GPU testing
torchpack dist-run -np 8 python tools/test.py \
    runs/run-5e3259ec/configs.yaml \
    runs/run-5e3259ec/epoch_20.pth \
    --eval bbox
```

#### Expected Waymo Results
```
Waymo Evaluation Results:
mAP: 0.3588
mATE: 0.4818
mASE: 0.5229
mAOE: 0.6159
mAVE: 0.0000
mAAE: 0.0000
NDS: 0.4308

Per-class results:
Object Class              AP        ATE       ASE       AOE       AVE       AAE
vehicle                   0.628     0.376     0.313     0.536     0.000     0.000
pedestrian                0.561     0.134     0.387     0.349     0.000     0.000
cyclist                   0.000     1.000     1.000     1.000     0.000     0.000
sign                      0.577     0.166     0.195     0.345     0.000     0.000
```

### nuScenes Testing

```bash
# Test with trained model
python tools/test.py \
    configs/nuscenes/det/transfusion/secfpn/camera+lidar/swint_v0p075/convfuser.yaml \
    checkpoints/nuscenes_model.pth \
    --eval bbox

# Multi-GPU testing
torchpack dist-run -np 8 python tools/test.py \
    configs/nuscenes/det/transfusion/secfpn/camera+lidar/swint_v0p075/convfuser.yaml \
    checkpoints/nuscenes_model.pth \
    --eval bbox
```

#### Expected nuScenes Results
```
mAP: 0.3588
mATE: 0.4818
mASE: 0.5229
mAOE: 0.6159
mAVE: 0.5402
mAAE: 0.3253
NDS: 0.4308

Per-class results:
Object Class            AP        ATE       ASE       AOE       AVE       AAE
car                     0.318     0.376     0.313     0.536     0.248     0.202
truck                   0.194     0.430     0.264     0.300     0.131     0.006
bus                     0.947     0.204     0.127     0.063     0.849     0.271
trailer                 0.000     1.000     1.000     1.000     1.000     1.000
construction_vehicle    0.000     1.000     1.000     1.000     1.000     1.000
pedestrian              0.805     0.134     0.387     0.349     0.241     0.120
motorcycle              0.039     0.421     0.466     0.950     0.061     0.004
bicycle                 0.467     0.166     0.195     0.345     0.791     0.000
traffic_cone            0.819     0.087     0.476     nan       nan       nan
barrier                 0.000     1.000     1.000     1.000     nan       nan
```

## ğŸ‘ï¸ Visualization

### Waymo Visualization

#### Ground Truth Visualization
```bash
# Visualize GT boxes
python tools/visualize.py \
    configs/waymo/det/transfusion/secfpn/camera+lidar/default.yaml \
    --mode gt \
    --split val \
    --out-dir viz_waymo_gt

# Visualize specific classes
python tools/visualize.py \
    configs/waymo/det/transfusion/secfpn/camera+lidar/default.yaml \
    --mode gt \
    --split val \
    --bbox-classes 0 1 \
    --out-dir viz_waymo_vehicle_pedestrian
```

#### Prediction Visualization
```bash
# Visualize model predictions
python tools/visualize.py \
    configs/waymo/det/transfusion/secfpn/camera+lidar/default.yaml \
    --mode pred \
    --checkpoint checkpoints/waymo_model.pth \
    --split val \
    --bbox-score 0.3 \
    --out-dir viz_waymo_pred
```

#### Class Mapping (Waymo)
- `0`: vehicle
- `1`: pedestrian
- `2`: cyclist
- `3`: sign

### nuScenes Visualization

#### Ground Truth Visualization
```bash
# Visualize GT boxes
python tools/visualize.py \
    configs/nuscenes/det/transfusion/secfpn/camera+lidar/swint_v0p075/convfuser.yaml \
    --mode gt \
    --split val \
    --out-dir viz_nuscenes_gt

# Visualize specific classes
python tools/visualize.py \
    configs/nuscenes/det/transfusion/secfpn/camera+lidar/swint_v0p075/convfuser.yaml \
    --mode gt \
    --split val \
    --bbox-classes 0 1 2 \
    --out-dir viz_nuscenes_car_truck_bus
```

#### Prediction Visualization
```bash
# Visualize model predictions
python tools/visualize.py \
    configs/nuscenes/det/transfusion/secfpn/camera+lidar/swint_v0p075/convfuser.yaml \
    --mode pred \
    --checkpoint checkpoints/nuscenes_model.pth \
    --split val \
    --bbox-score 0.3 \
    --out-dir viz_nuscenes_pred
```

#### Class Mapping (nuScenes)
- `0`: car
- `1`: truck
- `2`: construction_vehicle
- `3`: bus
- `4`: trailer
- `5`: barrier
- `6`: motorcycle
- `7`: bicycle
- `8`: pedestrian
- `9`: traffic_cone

### Visualization Output

Visualization results are saved in:
```
viz_output/
â”œâ”€â”€ camera-0/          # Front camera images with projected 3D boxes
â”œâ”€â”€ camera-1/          # Front-right camera images
â”œâ”€â”€ camera-2/          # Back camera images
â”œâ”€â”€ camera-3/          # Front-left camera images
â”œâ”€â”€ camera-4/          # Back-left camera images
â””â”€â”€ lidar/             # LiDAR Bird's Eye View with 3D boxes
```

**Note**: Camera visualization for Waymo currently has coordinate transformation issues. LiDAR Bird's Eye View visualization is recommended and fully functional.

## ğŸ“ Directory Structure

```
bevfusion/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ waymo/
â”‚   â”‚   â””â”€â”€ det/
â”‚   â”‚       â”œâ”€â”€ centerhead/
â”‚   â”‚       â””â”€â”€ transfusion/
â”‚   â””â”€â”€ nuscenes/
â”‚       â””â”€â”€ det/
â”‚           â”œâ”€â”€ centerhead/
â”‚           â””â”€â”€ transfusion/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ waymo/
â”‚   â””â”€â”€ nuscenes/
â”œâ”€â”€ mmdet3d/
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â”œâ”€â”€ waymo_dataset.py
â”‚   â”‚   â””â”€â”€ nuscenes_dataset.py
â”‚   â””â”€â”€ models/
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ test.py
â”‚   â””â”€â”€ visualize.py
â”œâ”€â”€ runs/                # Training outputs
â”‚   â””â”€â”€ run-xxxxxxxx/
â”‚       â”œâ”€â”€ configs.yaml
â”‚       â”œâ”€â”€ latest.pth
â”‚       â””â”€â”€ epoch_*.pth
â””â”€â”€ README.md
```

## ğŸ”§ Common Issues

### Waymo Dataset Issues
1. **Class mapping**: cyclist and sign classes may have low AP due to data distribution
2. **Camera visualization**: LiDAR2Image transformation issues - use LiDAR visualization instead
3. **Memory usage**: Reduce batch size if GPU memory is insufficient

### nuScenes Dataset Issues
1. **Data version**: Ensure you're using v1.0-trainval
2. **Map data**: Some metrics require map annotations
3. **Velocity**: nuScenes includes velocity information, Waymo does not

### Training Issues
1. **CUDA OOM**: Reduce batch size or image resolution
2. **Slow training**: Use multi-GPU training or reduce data augmentation
3. **NaN loss**: Check learning rate and data preprocessing

## ğŸ“š Additional Resources

- [Original BEVFusion Paper](https://arxiv.org/abs/2205.13542)
- [Waymo Open Dataset](https://waymo.com/open/)
- [nuScenes Dataset](https://www.nuscenes.org/)
- [MMDetection3D Documentation](https://mmdetection3d.readthedocs.io/)

## ğŸ¯ Performance Benchmarks

### Waymo (Camera + LiDAR)
- **mAP**: 35-45%
- **Training time**: ~2-3 days on 8x A100
- **Inference**: ~15 FPS

### nuScenes (Camera + LiDAR)
- **mAP**: 65-70%
- **NDS**: 70-75%
- **Training time**: ~1-2 days on 8x A100
- **Inference**: ~12 FPS

---

*This implementation includes improvements for Waymo dataset support and enhanced evaluation metrics.*